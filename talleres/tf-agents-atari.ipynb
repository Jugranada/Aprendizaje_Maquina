{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:39:20.405291Z","iopub.execute_input":"2025-12-11T16:39:20.405523Z","iopub.status.idle":"2025-12-11T16:39:22.848728Z","shell.execute_reply.started":"2025-12-11T16:39:20.405502Z","shell.execute_reply":"2025-12-11T16:39:22.847845Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import base64\nimport imageio\nimport IPython\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport pyvirtualdisplay\n\n# Esta línea inicia una pantalla virtual para que el juego pueda generar imágenes internamente.\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1400, 900))\ndisplay.start()\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_atari\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import q_network\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:42:10.898324Z","iopub.execute_input":"2025-12-11T16:42:10.899095Z","iopub.status.idle":"2025-12-11T16:42:25.108733Z","shell.execute_reply.started":"2025-12-11T16:42:10.899051Z","shell.execute_reply":"2025-12-11T16:42:25.107655Z"}},"outputs":[{"name":"stderr","text":"2025-12-11 16:42:11.734351: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-12-11 16:42:11.735150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-12-11 16:42:11.737296: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## **Instalación de Herramientas y el Juego**\nEn este proceso, descargamos todo lo necesario para que funcione el laboratorio.\n\nInstalamos las librerías de Inteligencia Artificial.\n\nImportante: Instalamos el emulador de Atari (ALE) y las \"ROMs\" (los archivos del juego), ya que Freeway es un juego de consola antigua","metadata":{}},{"cell_type":"code","source":"# Esta línea instala las herramientas básicas de TensorFlow y agentes.\n!pip install -q tf-agents[reverb]\n\n# Esta línea instala el emulador de juegos de Atari.\n!pip install -q gymnasium[atari]\n!pip install -q gymnasium[accept-rom-license]\n\n# Esta línea instala herramientas para manejar las imágenes del juego.\n!pip install -q imageio\n!pip install -q pyvirtualdisplay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:39:22.850367Z","iopub.execute_input":"2025-12-11T16:39:22.850825Z","iopub.status.idle":"2025-12-11T16:41:49.454110Z","shell.execute_reply.started":"2025-12-11T16:39:22.850801Z","shell.execute_reply":"2025-12-11T16:41:49.452643Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nblake3 1.0.8 requires typing-extensions>=4.6.0; python_full_version < \"3.12\", but you have typing-extensions 4.5.0 which is incompatible.\npyopenssl 25.3.0 requires typing-extensions>=4.9; python_version < \"3.13\" and python_version >= \"3.8\", but you have typing-extensions 4.5.0 which is incompatible.\npydantic-core 2.41.5 requires typing-extensions>=4.14.1, but you have typing-extensions 4.5.0 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nsigstore-models 0.0.5 requires typing-extensions>=4.14.1, but you have typing-extensions 4.5.0 which is incompatible.\nbetterproto 2.0.0b7 requires typing-extensions<5.0.0,>=4.7.1, but you have typing-extensions 4.5.0 which is incompatible.\ntyping-inspection 0.4.2 requires typing-extensions>=4.12.0, but you have typing-extensions 4.5.0 which is incompatible.\npydantic 2.12.4 requires typing-extensions>=4.14.1, but you have typing-extensions 4.5.0 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\ndocstring-to-markdown 0.17 requires typing_extensions>=4.6, but you have typing-extensions 4.5.0 which is incompatible.\nonnx 1.18.0 requires typing_extensions>=4.7.1, but you have typing-extensions 4.5.0 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.10.0 which is incompatible.\npytorch-lightning 2.5.5 requires typing-extensions>4.5.0, but you have typing-extensions 4.5.0 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nplum-dispatch 2.6.0 requires typing-extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\nalembic 1.17.1 requires typing-extensions>=4.12, but you have typing-extensions 4.5.0 which is incompatible.\npudb 2025.1.3 requires typing-extensions>=4.13, but you have typing-extensions 4.5.0 which is incompatible.\ngoogle-genai 1.48.0 requires typing-extensions<5.0.0,>=4.11.0, but you have typing-extensions 4.5.0 which is incompatible.\nopenai 2.7.1 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\njax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.3.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntypeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.5.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.1.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\nwandb 0.21.0 requires typing-extensions<5,>=4.8, but you have typing-extensions 4.5.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\noptree 0.16.0 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\nstarlette 0.47.2 requires typing-extensions>=4.10.0; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\nnibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\ntf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.1 which is incompatible.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.1 which is incompatible.\ntensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.1 which is incompatible.\naltair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\nfastapi 0.116.1 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\nsqlalchemy 2.0.41 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\nlangchain-core 0.3.72 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.18.0 requires shimmy>=1.2.1, but you have shimmy 0.2.1 which is incompatible.\ndopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Esta línea define cuántas veces va a entrenar.\nnum_iterations = 25000 \n\n# Pasos iniciales al azar.\ninitial_collect_steps = 2000 \n\n# Pasos por vuelta de entrenamiento.\ncollect_steps_per_iteration = 1\n\n# Tamaño de memoria.\nreplay_buffer_max_length = 100000\n\n# Tamaño del lote.\nbatch_size = 64\n\n\nlearning_rate = 2.5e-4\n\n# Intervalos de log visuales.\nlog_interval = 200\nnum_eval_episodes = 5\neval_interval = 10000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:42:58.374979Z","iopub.execute_input":"2025-12-11T16:42:58.375381Z","iopub.status.idle":"2025-12-11T16:42:58.380964Z","shell.execute_reply.started":"2025-12-11T16:42:58.375353Z","shell.execute_reply":"2025-12-11T16:42:58.380011Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Configuración del entorno Atari\n\n1. **Definición del juego**:  \n   Se selecciona el juego de Atari a entrenar, en este caso `Freeway-v4`.\n\n2. **Carga del entorno**:  \n   Se utiliza `suite_atari` para cargar automáticamente el entorno de Gym con configuraciones estándar:  \n   - Escalado a **grises**.  \n   - Apilamiento de **4 frames** para capturar movimiento.  \n   - Limite máximo de **2000 pasos por episodio**.  \n\n   Se crean dos entornos: uno para entrenamiento (`train_py_env`) y otro para evaluación (`eval_py_env`).\n\n3. **Conversión a TensorFlow**:  \n   Los entornos de Python (`PyEnvironment`) se convierten a entornos compatibles con TensorFlow (`TFPyEnvironment`) para integrarse con agentes de TF-Agents.\n\n4. **Especificaciones del entorno**:  \n   Se imprimen las **observaciones** y las **acciones** posibles, para entender la forma de los datos que manejará el agente.\n","metadata":{}},{"cell_type":"code","source":"# Esta línea define el nombre del juego.\nenv_name = 'Freeway-v4'\n\n# --- CARGA DEL ENTORNO ---\n# Usamos suite_atari que ya viene preparado para trabajar con gym 0.21.0\n# Incluye automáticamente el apilado de 4 frames y la escala de grises.\ntrain_py_env = suite_atari.load(\n    env_name,\n    max_episode_steps=2000,\n    gym_env_wrappers=suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)\n\neval_py_env = suite_atari.load(\n    env_name,\n    max_episode_steps=2000,\n    gym_env_wrappers=suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)\n\n# Convertimos a TensorFlow\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\nprint('Observation Spec:', train_env.observation_spec())\nprint('Action Spec:', train_env.action_spec())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:42:42.035464Z","iopub.execute_input":"2025-12-11T16:42:42.036397Z","iopub.status.idle":"2025-12-11T16:42:42.646523Z","shell.execute_reply.started":"2025-12-11T16:42:42.036362Z","shell.execute_reply":"2025-12-11T16:42:42.645747Z"}},"outputs":[{"name":"stderr","text":"A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n[Powered by Stella]\n","output_type":"stream"},{"name":"stdout","text":"Observation Spec: BoundedTensorSpec(shape=(84, 84, 4), dtype=tf.uint8, name='observation', minimum=array(0, dtype=uint8), maximum=array(255, dtype=uint8))\nAction Spec: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(2))\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Esta línea resetea el entorno para empezar de cero.\ntime_step = eval_env.reset()\n\n# Esta línea toma la imagen actual del juego (en formato de números).\nobservation = time_step.observation.numpy()[0]\n\n# Esta línea nos prepara un \"lienzo\" para dibujar.\nplt.figure(figsize=(5, 5))\n\n# Esta línea dibuja la imagen. Nota: Mostramos solo el primer frame de los 4 apilados.\n# Verás los carriles y quizás al pollo (un punto brillante) abajo.\nplt.imshow(observation[:, :, 0], cmap='gray')\nplt.title(\"Lo que ve el Agente (Freeway)\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:43:01.196625Z","iopub.execute_input":"2025-12-11T16:43:01.196989Z","iopub.status.idle":"2025-12-11T16:43:01.777791Z","shell.execute_reply.started":"2025-12-11T16:43:01.196964Z","shell.execute_reply":"2025-12-11T16:43:01.776918Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 500x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# Definición de la Red Q para el agente\n\n1. **Arquitectura convolucional**:  \n   Se define la estructura de la red neuronal que procesará las imágenes del juego:  \n   - `conv_layer_params`: tres capas convolucionales con filtros, tamaño de kernel y stride específicos.  \n   - `fc_layer_params`: capa totalmente conectada con 512 unidades.\n\n2. **Preprocesamiento de observaciones**:  \n   - Se crea una capa Lambda para **normalizar** los pixeles de enteros (0-255) a decimales (0-1).\n\n3. **Creación de la red Q**:  \n   - `q_network.QNetwork` toma las especificaciones del entorno y las capas definidas.  \n   - Esta red se usará para estimar los **valores Q** de cada acción en un estado dado.\n\n4. **Inicialización manual**:  \n   - Se crea un `dummy_input` con la forma de la observación para inicializar los pesos de la red.  \n   - Esto asegura que todas las dimensiones estén correctas antes de entrenar el agente.\n","metadata":{}},{"cell_type":"code","source":"# Definimos la estructura visual\nconv_layer_params = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]\nfc_layer_params = (512,)\n\n# Capa de preprocesamiento (Enteros -> Decimales)\npreprocessing_layer = tf.keras.layers.Lambda(\n    lambda obs: tf.cast(obs, tf.float32) / 255.0)\n\n# Creamos la red Q\nq_net = q_network.QNetwork(\n    train_env.observation_spec(),\n    train_env.action_spec(),\n    preprocessing_layers=preprocessing_layer,\n    conv_layer_params=conv_layer_params,\n    fc_layer_params=fc_layer_params)\n\n\ninput_shape = (1,) + train_env.observation_spec().shape\ndummy_input = tf.zeros(input_shape, dtype=tf.uint8)\n\n\nprint(f\"Inicializando red manualmente con forma: {input_shape}...\")\n_ = q_net(dummy_input)\nprint(\"¡Red inicializada y dimensiones fijadas correctamente!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:43:04.302240Z","iopub.execute_input":"2025-12-11T16:43:04.302610Z","iopub.status.idle":"2025-12-11T16:43:04.516213Z","shell.execute_reply.started":"2025-12-11T16:43:04.302585Z","shell.execute_reply":"2025-12-11T16:43:04.515201Z"}},"outputs":[{"name":"stdout","text":"Inicializando red manualmente con forma: (1, 84, 84, 4)...\n¡Red inicializada y dimensiones fijadas correctamente!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Configuración del agente DQN\n\n1. **Optimizador**:  \n   - Se utiliza `Adam` con una tasa de aprendizaje definida y `clipnorm=10.0` para limitar gradientes grandes y estabilizar el entrenamiento.\n\n2. **Contador de pasos de entrenamiento**:  \n   - `train_step_counter` lleva la cuenta de los pasos realizados durante el entrenamiento del agente.\n\n3. **Definición del agente DQN**:  \n   - `DqnAgent` toma las especificaciones del entorno y la red Q previamente definida.  \n   - Se usa la **Huber Loss** en lugar de la pérdida cuadrática para ser más resistente a errores grandes.  \n   - Se asocia con el optimizador y el contador de pasos.\n\n4. **Inicialización y políticas**:  \n   - `agent.initialize()` prepara todos los parámetros internos del agente.  \n   - `eval_policy`: política usada para evaluación del agente.  \n   - `collect_policy`: política usada para recolectar experiencias durante el entrenamiento.\n","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=10.0)\n\ntrain_step_counter = tf.Variable(0)\n\n# Usamos DQN estándar\nagent = dqn_agent.DqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    # Usamos Huber Loss en lugar de Squared Loss porque es más resistente a errores grandes\n    td_errors_loss_fn=common.element_wise_huber_loss,\n    train_step_counter=train_step_counter)\n\nagent.initialize()\n\neval_policy = agent.policy\ncollect_policy = agent.collect_policy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:43:35.212780Z","iopub.execute_input":"2025-12-11T16:43:35.213152Z","iopub.status.idle":"2025-12-11T16:43:35.299477Z","shell.execute_reply.started":"2025-12-11T16:43:35.213127Z","shell.execute_reply":"2025-12-11T16:43:35.298555Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Función de evaluación y Replay Buffer\n\n1. **Cálculo del puntaje promedio**:  \n   - `compute_avg_return` ejecuta varias partidas (`num_episodes`) usando una política dada.  \n   - Suma las recompensas de cada episodio y calcula el promedio.  \n   - Permite medir el desempeño del agente de manera consistente.\n\n2. **Replay Buffer**:  \n   - `TFUniformReplayBuffer` almacena las experiencias del agente (estado, acción, recompensa, siguiente estado).  \n   - Facilita el **entrenamiento por lotes** (batch training) y rompe la correlación temporal entre experiencias, mejorando la estabilidad del aprendizaje.  \n   - Se define con la especificación de datos del agente, tamaño de batch y longitud máxima del buffer.\n","metadata":{}},{"cell_type":"code","source":"# Esta función calcula el puntaje promedio jugando varias partidas.\ndef compute_avg_return(environment, policy, num_episodes=10):\n  total_return = 0.0\n  for _ in range(num_episodes):\n    time_step = environment.reset()\n    episode_return = 0.0\n    while not time_step.is_last():\n      action_step = policy.action(time_step)\n      time_step = environment.step(action_step.action)\n      episode_return += time_step.reward\n    total_return += episode_return\n  avg_return = total_return / num_episodes\n  return avg_return.numpy()[0]\n\n# Esta línea crea el buffer de memoria para guardar las partidas y aprender de ellas.\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=replay_buffer_max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:43:37.287625Z","iopub.execute_input":"2025-12-11T16:43:37.288457Z","iopub.status.idle":"2025-12-11T16:43:38.893358Z","shell.execute_reply.started":"2025-12-11T16:43:37.288427Z","shell.execute_reply":"2025-12-11T16:43:38.892541Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Creación de entornos robustos y recolección de datos\n\n1. **Wrapper de compatibilidad `AtariCompatWrapper`**:  \n   - Adapta la API nueva de Gym a la antigua, asegurando que `step` y `reset` devuelvan los formatos esperados por TF-Agents.  \n   - Maneja diferencias en los valores retornados (`terminated`, `truncated`) y observaciones (`obs, info`).\n\n2. **Función `create_robust_atari_env`**:  \n   - Construye un entorno de Atari robusto paso a paso:  \n     1. Desenvuelve el entorno original (`unwrapped`) para eliminar wrappers problemáticos.  \n     2. Aplica el wrapper de compatibilidad.  \n     3. Añade límite de tiempo al episodio.  \n     4. Preprocesa la imagen (escala de grises, resize a 84x84).  \n     5. Apila 4 frames consecutivos para capturar movimiento.  \n     6. Convierte el entorno a un `TFPyEnvironment` compatible con TF-Agents.\n\n3. **Recolección de datos inicial**:  \n   - `DynamicStepDriver` recorre el entorno usando la política de colección (`collect_policy`) y almacena las transiciones en el `replay_buffer`.  \n   - `num_steps=initial_collect_steps` determina cuántos pasos iniciales se recolectan antes de entrenar.\n\n4. **Preparación del dataset de entrenamiento**:  \n   - Se crea un dataset a partir del `replay_buffer` con lotes de tamaño `batch_size` y secuencias de `num_steps=2`.  \n   - `prefetch(3)` permite que TensorFlow cargue datos de manera eficiente mientras se entrena el agente.  \n   - Se obtiene un iterador para extraer lotes de manera secuencial durante el entrenamiento.\n","metadata":{}},{"cell_type":"code","source":"import gym\nfrom tf_agents.environments import gym_wrapper\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import atari_preprocessing\nfrom tf_agents.environments import atari_wrappers\nfrom tf_agents.drivers import dynamic_step_driver\n\n\n\nclass AtariCompatWrapper(gym.Wrapper):\n    def step(self, action):\n        val = self.env.step(action)\n        # Si devuelve 5 valores (nuevo API), los convertimos a 4 (viejo API).\n        if len(val) == 5:\n            obs, reward, terminated, truncated, info = val\n            return obs, reward, terminated or truncated, info\n        return val\n\n    def reset(self, **kwargs):\n        val = self.env.reset(**kwargs)\n        # Si devuelve tupla (obs, info), nos quedamos solo con obs.\n        if isinstance(val, tuple) and len(val) == 2:\n            return val[0]\n        return val\n\ndef create_robust_atari_env(env_name):\n    # 1. Creamos el entorno y le quitamos TODOS los wrappers automáticos (.unwrapped)\n    # Esto elimina al culpable del error (OrderEnforcing) que estaba fallando antes.\n    raw_env = gym.make(env_name).unwrapped\n    \n    # 2. Aplicamos nuestro parche inmediatamente al núcleo\n    env = AtariCompatWrapper(raw_env)\n    \n    # 3. Re-aplicamos el límite de tiempo (vital para que los episodios terminen)\n    env = gym.wrappers.TimeLimit(env, max_episode_steps=2000)\n    \n    # 4. Aplicamos el preprocesamiento estándar de Atari (Gris, Resize 84x84)\n    env = atari_preprocessing.AtariPreprocessing(env)\n    \n    # 5. Apilamos los frames (para detectar velocidad y dirección)\n    env = atari_wrappers.FrameStack4(env)\n    \n    # 6. Convertimos a TF-Agents\n    return gym_wrapper.GymWrapper(env)\n\n# --- CARGA DE ENTORNOS ---\nprint(f\"Reconstruyendo entorno {env_name} desde el núcleo...\")\npy_env_train = create_robust_atari_env(env_name)\npy_env_eval = create_robust_atari_env(env_name)\n\n# Convertimos a TensorFlow Environment\ntrain_env = tf_py_environment.TFPyEnvironment(py_env_train)\neval_env = tf_py_environment.TFPyEnvironment(py_env_eval)\n\n# ---------------------------------------------------------\n# Ejecución de la Recolección\n# ---------------------------------------------------------\n\n# Creamos el conductor\ncollect_driver = dynamic_step_driver.DynamicStepDriver(\n    train_env,\n    collect_policy,\n    observers=[replay_buffer.add_batch],\n    num_steps=initial_collect_steps)\n\nprint(\"Iniciando recolección de datos...\")\ncollect_driver.run()\nprint(\"¡Recolección finalizada con éxito!\")\n\n# Preparamos los datos para el entrenamiento\ndataset = replay_buffer.as_dataset(\n    num_parallel_calls=3,\n    sample_batch_size=batch_size,\n    num_steps=2).prefetch(3)\n\niterator = iter(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:43:46.224413Z","iopub.execute_input":"2025-12-11T16:43:46.225530Z","iopub.status.idle":"2025-12-11T16:44:35.289411Z","shell.execute_reply.started":"2025-12-11T16:43:46.225495Z","shell.execute_reply":"2025-12-11T16:44:35.288557Z"}},"outputs":[{"name":"stdout","text":"Reconstruyendo entorno Freeway-v4 desde el núcleo...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment Freeway-v4 is out of date. You should consider upgrading to version `v5` with the environment ID `ALE/Freeway-v5`.\u001b[0m\n  logger.warn(\n","output_type":"stream"},{"name":"stdout","text":"Iniciando recolección de datos...\n¡Recolección finalizada con éxito!\nWARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `as_dataset(..., single_deterministic_pass=False) instead.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `as_dataset(..., single_deterministic_pass=False) instead.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Entrenamiento del agente DQN\n\n1. **Recolección de datos rápida**:  \n   - Se crea un `DynamicStepDriver` para recolectar solo `collect_steps_per_iteration` pasos por iteración, acelerando la recolección y evitando recorrer episodios completos de 2000 pasos.\n\n2. **Configuración de impresión y evaluación**:  \n   - `log_interval` define cada cuántos pasos se imprime el progreso.  \n   - `eval_interval` define cada cuántos pasos se evalúa el agente en el entorno de evaluación.  \n\n3. **Optimización y preparación**:  \n   - `agent.train` se envuelve en `common.function` para mayor eficiencia.  \n   - Se reinicia el contador de pasos y se evalúa el retorno promedio inicial antes de entrenar.  \n\n4. **Bucle principal de entrenamiento**:  \n   - **Recolectar experiencia**: un paso rápido en el entorno usando `collect_policy`.  \n   - **Extraer batch** del `replay_buffer` con `next(iterator)`.  \n   - **Entrenar la red Q** usando el batch y actualizar pesos.  \n   - **Impresión de progreso**: se muestra el loss y tiempo transcurrido cada 100 pasos y se logea oficialmente cada `log_interval`.  \n   - **Evaluación periódica**: se calcula el retorno promedio cada `eval_interval` pasos y se guarda para seguimiento.  \n\n5. **Finalización**:  \n   - Al terminar el bucle, se confirma que el entrenamiento ha finalizado exitosamente.\n","metadata":{}},{"cell_type":"code","source":"import time\nimport sys\n\n\ncollect_driver_training = dynamic_step_driver.DynamicStepDriver(\n    train_env,\n    collect_policy,\n    observers=[replay_buffer.add_batch],\n    num_steps=collect_steps_per_iteration) # <-- Aquí está la clave: 1 paso, no 2000.\n\n# 2. Configuramos intervalos de impresión para calmar la ansiedad\nlog_interval = 100   # Imprimir error cada 100 pasos\neval_interval = 1000 # Evaluar cada 1000 pasos\n\n# Optimizamos\nagent.train = common.function(agent.train)\nagent.train_step_counter.assign(0)\n\n# Reset de métricas\nprint(\"Evaluando agente antes de empezar...\")\navg_return = compute_avg_return(eval_env, eval_policy, num_eval_episodes)\nreturns = [avg_return]\nprint(f'Retorno inicial: {avg_return}')\n\nprint(\"--- INICIO DEL ENTRENAMIENTO VELOZ ---\")\nstart_time = time.time()\n\nfor _ in range(num_iterations):\n\n  # 1. Recolectar (¡Ahora es rápido! Solo 1 paso)\n  collect_driver_training.run()\n\n  # 2. Muestrear datos\n  experience, _ = next(iterator)\n\n  # 3. Entrenar\n  train_loss = agent.train(experience).loss\n\n  # Obtener paso actual\n  step = agent.train_step_counter.numpy()\n\n  # --- IMPRESIÓN CONSTANTE DE PROGRESO ---\n  # Imprimimos cada 10 pasos para que veas que no está colgado\n  if step % 100 == 0:\n      elapsed = time.time() - start_time\n      # Usamos sys.stdout para forzar la impresión inmediata\n      print(f'\\rIteración: {step} | Loss: {train_loss:.3f} | Tiempo: {elapsed:.1f}s', end='')\n\n  # Guardar log oficial cada log_interval\n  if step % log_interval == 0:\n      print(f'\\n[Log] Paso {step}: Loss final = {train_loss:.4f}')\n\n  # Evaluar rendimiento\n  if step % eval_interval == 0:\n      avg_return = compute_avg_return(eval_env, eval_policy, num_eval_episodes)\n      print(f'\\n--- EVALUACIÓN Paso {step}: Puntos Promedio = {avg_return} ---')\n      returns.append(avg_return)\n\nprint(\"\\n¡Entrenamiento finalizado!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T16:44:35.290876Z","iopub.execute_input":"2025-12-11T16:44:35.291205Z"}},"outputs":[{"name":"stdout","text":"Evaluando agente antes de empezar...\nRetorno inicial: 16.200000762939453\n--- INICIO DEL ENTRENAMIENTO VELOZ ---\nWARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\nInstructions for updating:\nback_prop=False is deprecated. Consider using tf.stop_gradient instead.\nInstead of:\nresults = tf.foldr(fn, elems, back_prop=False)\nUse:\nresults = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\nInstructions for updating:\nback_prop=False is deprecated. Consider using tf.stop_gradient instead.\nInstead of:\nresults = tf.foldr(fn, elems, back_prop=False)\nUse:\nresults = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n","output_type":"stream"},{"name":"stdout","text":"Iteración: 100 | Loss: 56.098 | Tiempo: 13.1s\n[Log] Paso 100: Loss final = 56.0982\nIteración: 200 | Loss: 2388.888 | Tiempo: 24.3s\n[Log] Paso 200: Loss final = 2388.8877\nIteración: 300 | Loss: 16800.875 | Tiempo: 35.1s\n[Log] Paso 300: Loss final = 16800.8750\nIteración: 400 | Loss: 57302.062 | Tiempo: 46.1s\n[Log] Paso 400: Loss final = 57302.0625\nIteración: 500 | Loss: 1070182.750 | Tiempo: 56.8s\n[Log] Paso 500: Loss final = 1070182.7500\nIteración: 600 | Loss: 371622.750 | Tiempo: 67.8s\n[Log] Paso 600: Loss final = 371622.7500\nIteración: 700 | Loss: 756366.000 | Tiempo: 78.7s\n[Log] Paso 700: Loss final = 756366.0000\nIteración: 800 | Loss: 13157213.000 | Tiempo: 89.5s\n[Log] Paso 800: Loss final = 13157213.0000\nIteración: 900 | Loss: 21903512.000 | Tiempo: 100.4s\n[Log] Paso 900: Loss final = 21903512.0000\nIteración: 1000 | Loss: 677681.500 | Tiempo: 111.3s\n[Log] Paso 1000: Loss final = 677681.5000\n\n--- EVALUACIÓN Paso 1000: Puntos Promedio = 0.0 ---\nIteración: 1100 | Loss: 9080232.000 | Tiempo: 166.9s\n[Log] Paso 1100: Loss final = 9080232.0000\nIteración: 1200 | Loss: 7318640.000 | Tiempo: 177.7s\n[Log] Paso 1200: Loss final = 7318640.0000\nIteración: 1300 | Loss: 4196248.000 | Tiempo: 188.5s\n[Log] Paso 1300: Loss final = 4196248.0000\nIteración: 1400 | Loss: 164997232.000 | Tiempo: 199.5s\n[Log] Paso 1400: Loss final = 164997232.0000\nIteración: 1500 | Loss: 4912272.000 | Tiempo: 210.4s\n[Log] Paso 1500: Loss final = 4912272.0000\nIteración: 1600 | Loss: 5755776.000 | Tiempo: 221.7s\n[Log] Paso 1600: Loss final = 5755776.0000\nIteración: 1700 | Loss: 12572832.000 | Tiempo: 232.7s\n[Log] Paso 1700: Loss final = 12572832.0000\nIteración: 1800 | Loss: 13672704.000 | Tiempo: 243.9s\n[Log] Paso 1800: Loss final = 13672704.0000\nIteración: 1900 | Loss: 12608256.000 | Tiempo: 255.1s\n[Log] Paso 1900: Loss final = 12608256.0000\nIteración: 2000 | Loss: 32442688.000 | Tiempo: 266.2s\n[Log] Paso 2000: Loss final = 32442688.0000\n\n--- EVALUACIÓN Paso 2000: Puntos Promedio = 16.200000762939453 ---\nIteración: 2100 | Loss: 25915136.000 | Tiempo: 322.9s\n[Log] Paso 2100: Loss final = 25915136.0000\nIteración: 2200 | Loss: 1567936640.000 | Tiempo: 334.1s\n[Log] Paso 2200: Loss final = 1567936640.0000\nIteración: 2300 | Loss: 35427840.000 | Tiempo: 345.1s\n[Log] Paso 2300: Loss final = 35427840.0000\nIteración: 2400 | Loss: 47832064.000 | Tiempo: 356.2s\n[Log] Paso 2400: Loss final = 47832064.0000\nIteración: 2500 | Loss: 64992256.000 | Tiempo: 367.2s\n[Log] Paso 2500: Loss final = 64992256.0000\nIteración: 2600 | Loss: 75288832.000 | Tiempo: 378.0s\n[Log] Paso 2600: Loss final = 75288832.0000\nIteración: 2700 | Loss: 79645440.000 | Tiempo: 388.8s\n[Log] Paso 2700: Loss final = 79645440.0000\nIteración: 2800 | Loss: 132224512.000 | Tiempo: 399.6s\n[Log] Paso 2800: Loss final = 132224512.0000\nIteración: 2900 | Loss: 144857600.000 | Tiempo: 410.4s\n[Log] Paso 2900: Loss final = 144857600.0000\nIteración: 3000 | Loss: 7113329664.000 | Tiempo: 421.2s\n[Log] Paso 3000: Loss final = 7113329664.0000\n\n--- EVALUACIÓN Paso 3000: Puntos Promedio = 0.0 ---\nIteración: 3100 | Loss: 192609280.000 | Tiempo: 476.5s\n[Log] Paso 3100: Loss final = 192609280.0000\nIteración: 3200 | Loss: 201439232.000 | Tiempo: 487.3s\n[Log] Paso 3200: Loss final = 201439232.0000\nIteración: 3300 | Loss: 301670400.000 | Tiempo: 498.0s\n[Log] Paso 3300: Loss final = 301670400.0000\nIteración: 3400 | Loss: 269559808.000 | Tiempo: 508.8s\n[Log] Paso 3400: Loss final = 269559808.0000\nIteración: 3500 | Loss: 383514624.000 | Tiempo: 519.6s\n[Log] Paso 3500: Loss final = 383514624.0000\nIteración: 3600 | Loss: 355406848.000 | Tiempo: 530.4s\n[Log] Paso 3600: Loss final = 355406848.0000\nIteración: 3700 | Loss: 18537027584.000 | Tiempo: 541.1s\n[Log] Paso 3700: Loss final = 18537027584.0000\nIteración: 3800 | Loss: 740317184.000 | Tiempo: 551.8s\n[Log] Paso 3800: Loss final = 740317184.0000\nIteración: 3900 | Loss: 573913088.000 | Tiempo: 562.6s\n[Log] Paso 3900: Loss final = 573913088.0000\nIteración: 4000 | Loss: 593788928.000 | Tiempo: 573.2s\n[Log] Paso 4000: Loss final = 593788928.0000\n\n--- EVALUACIÓN Paso 4000: Puntos Promedio = 0.0 ---\nIteración: 4100 | Loss: 778006528.000 | Tiempo: 627.8s\n[Log] Paso 4100: Loss final = 778006528.0000\nIteración: 4200 | Loss: 699701248.000 | Tiempo: 638.6s\n[Log] Paso 4200: Loss final = 699701248.0000\nIteración: 4300 | Loss: 717017088.000 | Tiempo: 649.2s\n[Log] Paso 4300: Loss final = 717017088.0000\nIteración: 4400 | Loss: 811778048.000 | Tiempo: 659.9s\n[Log] Paso 4400: Loss final = 811778048.0000\nIteración: 4500 | Loss: 960344064.000 | Tiempo: 670.7s\n[Log] Paso 4500: Loss final = 960344064.0000\nIteración: 4600 | Loss: 1132584960.000 | Tiempo: 681.4s\n[Log] Paso 4600: Loss final = 1132584960.0000\nIteración: 4700 | Loss: 56763244544.000 | Tiempo: 692.1s\n[Log] Paso 4700: Loss final = 56763244544.0000\nIteración: 4800 | Loss: 1289994240.000 | Tiempo: 702.8s\n[Log] Paso 4800: Loss final = 1289994240.0000\nIteración: 4900 | Loss: 1312501760.000 | Tiempo: 713.4s\n[Log] Paso 4900: Loss final = 1312501760.0000\nIteración: 5000 | Loss: 1617068032.000 | Tiempo: 724.2s\n[Log] Paso 5000: Loss final = 1617068032.0000\n\n--- EVALUACIÓN Paso 5000: Puntos Promedio = 0.0 ---\nIteración: 5100 | Loss: 80879058944.000 | Tiempo: 778.5s\n[Log] Paso 5100: Loss final = 80879058944.0000\nIteración: 5200 | Loss: 87329275904.000 | Tiempo: 789.1s\n[Log] Paso 5200: Loss final = 87329275904.0000\nIteración: 5300 | Loss: 1726218240.000 | Tiempo: 799.8s\n[Log] Paso 5300: Loss final = 1726218240.0000\nIteración: 5400 | Loss: 1906606080.000 | Tiempo: 810.5s\n[Log] Paso 5400: Loss final = 1906606080.0000\nIteración: 5500 | Loss: 1937948672.000 | Tiempo: 821.2s\n[Log] Paso 5500: Loss final = 1937948672.0000\nIteración: 5600 | Loss: 2177802240.000 | Tiempo: 832.0s\n[Log] Paso 5600: Loss final = 2177802240.0000\nIteración: 5700 | Loss: 2215206912.000 | Tiempo: 842.6s\n[Log] Paso 5700: Loss final = 2215206912.0000\nIteración: 5800 | Loss: 2284503040.000 | Tiempo: 853.1s\n[Log] Paso 5800: Loss final = 2284503040.0000\nIteración: 5900 | Loss: 155168423936.000 | Tiempo: 863.7s\n[Log] Paso 5900: Loss final = 155168423936.0000\nIteración: 6000 | Loss: 3612442624.000 | Tiempo: 874.2s\n[Log] Paso 6000: Loss final = 3612442624.0000\n\n--- EVALUACIÓN Paso 6000: Puntos Promedio = 16.200000762939453 ---\nIteración: 6100 | Loss: 4946100224.000 | Tiempo: 928.5s\n[Log] Paso 6100: Loss final = 4946100224.0000\nIteración: 6200 | Loss: 3104358400.000 | Tiempo: 939.2s\n[Log] Paso 6200: Loss final = 3104358400.0000\nIteración: 6300 | Loss: 4075241472.000 | Tiempo: 949.8s\n[Log] Paso 6300: Loss final = 4075241472.0000\nIteración: 6400 | Loss: 4579885056.000 | Tiempo: 960.4s\n[Log] Paso 6400: Loss final = 4579885056.0000\nIteración: 6500 | Loss: 4316594176.000 | Tiempo: 971.1s\n[Log] Paso 6500: Loss final = 4316594176.0000\nIteración: 6600 | Loss: 4403494912.000 | Tiempo: 981.8s\n[Log] Paso 6600: Loss final = 4403494912.0000\nIteración: 6700 | Loss: 5136744448.000 | Tiempo: 992.5s\n[Log] Paso 6700: Loss final = 5136744448.0000\nIteración: 6800 | Loss: 5689835520.000 | Tiempo: 1003.2s\n[Log] Paso 6800: Loss final = 5689835520.0000\nIteración: 6900 | Loss: 6664552448.000 | Tiempo: 1014.0s\n[Log] Paso 6900: Loss final = 6664552448.0000\nIteración: 7000 | Loss: 5366677504.000 | Tiempo: 1024.7s\n[Log] Paso 7000: Loss final = 5366677504.0000\n\n--- EVALUACIÓN Paso 7000: Puntos Promedio = 16.0 ---\nIteración: 7100 | Loss: 5143887872.000 | Tiempo: 1079.5s\n[Log] Paso 7100: Loss final = 5143887872.0000\nIteración: 7200 | Loss: 8202878976.000 | Tiempo: 1090.3s\n[Log] Paso 7200: Loss final = 8202878976.0000\nIteración: 7300 | Loss: 6824919040.000 | Tiempo: 1101.1s\n[Log] Paso 7300: Loss final = 6824919040.0000\nIteración: 7400 | Loss: 8489500672.000 | Tiempo: 1112.0s\n[Log] Paso 7400: Loss final = 8489500672.0000\nIteración: 7500 | Loss: 11627069440.000 | Tiempo: 1122.9s\n[Log] Paso 7500: Loss final = 11627069440.0000\nIteración: 7600 | Loss: 10235609088.000 | Tiempo: 1133.7s\n[Log] Paso 7600: Loss final = 10235609088.0000\nIteración: 7700 | Loss: 10916823040.000 | Tiempo: 1144.5s\n[Log] Paso 7700: Loss final = 10916823040.0000\nIteración: 7800 | Loss: 10667982848.000 | Tiempo: 1155.3s\n[Log] Paso 7800: Loss final = 10667982848.0000\nIteración: 7900 | Loss: 9710600192.000 | Tiempo: 1166.1s\n[Log] Paso 7900: Loss final = 9710600192.0000\nIteración: 8000 | Loss: 621642973184.000 | Tiempo: 1176.9s\n[Log] Paso 8000: Loss final = 621642973184.0000\n\n--- EVALUACIÓN Paso 8000: Puntos Promedio = 0.0 ---\nIteración: 8100 | Loss: 13555269632.000 | Tiempo: 1233.0s\n[Log] Paso 8100: Loss final = 13555269632.0000\nIteración: 8200 | Loss: 12988186624.000 | Tiempo: 1243.9s\n[Log] Paso 8200: Loss final = 12988186624.0000\nIteración: 8300 | Loss: 14448394240.000 | Tiempo: 1254.7s\n[Log] Paso 8300: Loss final = 14448394240.0000\nIteración: 8400 | Loss: 15711600640.000 | Tiempo: 1266.0s\n[Log] Paso 8400: Loss final = 15711600640.0000\nIteración: 8500 | Loss: 17081106432.000 | Tiempo: 1277.0s\n[Log] Paso 8500: Loss final = 17081106432.0000\nIteración: 8600 | Loss: 16167272448.000 | Tiempo: 1288.3s\n[Log] Paso 8600: Loss final = 16167272448.0000\nIteración: 8700 | Loss: 19396034560.000 | Tiempo: 1299.1s\n[Log] Paso 8700: Loss final = 19396034560.0000\nIteración: 8800 | Loss: 21584216064.000 | Tiempo: 1309.9s\n[Log] Paso 8800: Loss final = 21584216064.0000\nIteración: 8900 | Loss: 21446721536.000 | Tiempo: 1320.7s\n[Log] Paso 8900: Loss final = 21446721536.0000\nIteración: 9000 | Loss: 21617573888.000 | Tiempo: 1331.6s\n[Log] Paso 9000: Loss final = 21617573888.0000\n\n--- EVALUACIÓN Paso 9000: Puntos Promedio = 16.200000762939453 ---\nIteración: 9100 | Loss: 1127020232704.000 | Tiempo: 1386.5s\n[Log] Paso 9100: Loss final = 1127020232704.0000\nIteración: 9200 | Loss: 25928269824.000 | Tiempo: 1397.2s\n[Log] Paso 9200: Loss final = 25928269824.0000\nIteración: 9300 | Loss: 25813975040.000 | Tiempo: 1408.1s\n[Log] Paso 9300: Loss final = 25813975040.0000\nIteración: 9400 | Loss: 30730616832.000 | Tiempo: 1419.0s\n[Log] Paso 9400: Loss final = 30730616832.0000\nIteración: 9500 | Loss: 30425743360.000 | Tiempo: 1429.9s\n[Log] Paso 9500: Loss final = 30425743360.0000\nIteración: 9600 | Loss: 1427658768384.000 | Tiempo: 1440.9s\n[Log] Paso 9600: Loss final = 1427658768384.0000\nIteración: 9700 | Loss: 31544180736.000 | Tiempo: 1451.7s\n[Log] Paso 9700: Loss final = 31544180736.0000\nIteración: 9800 | Loss: 28813295616.000 | Tiempo: 1462.7s\n[Log] Paso 9800: Loss final = 28813295616.0000\nIteración: 9900 | Loss: 37727633408.000 | Tiempo: 1473.4s\n[Log] Paso 9900: Loss final = 37727633408.0000\nIteración: 10000 | Loss: 36458070016.000 | Tiempo: 1484.2s\n[Log] Paso 10000: Loss final = 36458070016.0000\n\n--- EVALUACIÓN Paso 10000: Puntos Promedio = 16.399999618530273 ---\nIteración: 10100 | Loss: 1791869321216.000 | Tiempo: 1539.4s\n[Log] Paso 10100: Loss final = 1791869321216.0000\nIteración: 10200 | Loss: 32010403840.000 | Tiempo: 1550.3s\n[Log] Paso 10200: Loss final = 32010403840.0000\nIteración: 10300 | Loss: 38818545664.000 | Tiempo: 1561.3s\n[Log] Paso 10300: Loss final = 38818545664.0000\nIteración: 10400 | Loss: 41909223424.000 | Tiempo: 1572.4s\n[Log] Paso 10400: Loss final = 41909223424.0000\nIteración: 10500 | Loss: 2132113358848.000 | Tiempo: 1583.5s\n[Log] Paso 10500: Loss final = 2132113358848.0000\nIteración: 10600 | Loss: 44124078080.000 | Tiempo: 1594.6s\n[Log] Paso 10600: Loss final = 44124078080.0000\nIteración: 10700 | Loss: 42367451136.000 | Tiempo: 1606.1s\n[Log] Paso 10700: Loss final = 42367451136.0000\nIteración: 10800 | Loss: 47872475136.000 | Tiempo: 1617.6s\n[Log] Paso 10800: Loss final = 47872475136.0000\nIteración: 10900 | Loss: 65411743744.000 | Tiempo: 1629.7s\n[Log] Paso 10900: Loss final = 65411743744.0000\nIteración: 11000 | Loss: 51679854592.000 | Tiempo: 1641.5s\n[Log] Paso 11000: Loss final = 51679854592.0000\n\n--- EVALUACIÓN Paso 11000: Puntos Promedio = 16.399999618530273 ---\nIteración: 11100 | Loss: 2676759986176.000 | Tiempo: 1699.1s\n[Log] Paso 11100: Loss final = 2676759986176.0000\nIteración: 11200 | Loss: 63259279360.000 | Tiempo: 1710.4s\n[Log] Paso 11200: Loss final = 63259279360.0000\nIteración: 11300 | Loss: 65669431296.000 | Tiempo: 1721.6s\n[Log] Paso 11300: Loss final = 65669431296.0000\nIteración: 11400 | Loss: 51371311104.000 | Tiempo: 1732.8s\n[Log] Paso 11400: Loss final = 51371311104.0000\nIteración: 11500 | Loss: 61685628928.000 | Tiempo: 1743.8s\n[Log] Paso 11500: Loss final = 61685628928.0000\nIteración: 11600 | Loss: 3249061756928.000 | Tiempo: 1754.9s\n[Log] Paso 11600: Loss final = 3249061756928.0000\nIteración: 11700 | Loss: 70830260224.000 | Tiempo: 1765.8s\n[Log] Paso 11700: Loss final = 70830260224.0000\nIteración: 11800 | Loss: 72961753088.000 | Tiempo: 1776.6s\n[Log] Paso 11800: Loss final = 72961753088.0000\nIteración: 11900 | Loss: 3622777389056.000 | Tiempo: 1787.5s\n[Log] Paso 11900: Loss final = 3622777389056.0000\nIteración: 12000 | Loss: 75582668800.000 | Tiempo: 1798.4s\n[Log] Paso 12000: Loss final = 75582668800.0000\n\n--- EVALUACIÓN Paso 12000: Puntos Promedio = 16.399999618530273 ---\nIteración: 12100 | Loss: 76220989440.000 | Tiempo: 1853.4s\n[Log] Paso 12100: Loss final = 76220989440.0000\nIteración: 12200 | Loss: 83890012160.000 | Tiempo: 1864.2s\n[Log] Paso 12200: Loss final = 83890012160.0000\nIteración: 12300 | Loss: 85306376192.000 | Tiempo: 1874.9s\n[Log] Paso 12300: Loss final = 85306376192.0000\nIteración: 12400 | Loss: 98833006592.000 | Tiempo: 1885.5s\n[Log] Paso 12400: Loss final = 98833006592.0000\nIteración: 12500 | Loss: 81219551232.000 | Tiempo: 1896.3s\n[Log] Paso 12500: Loss final = 81219551232.0000\nIteración: 12600 | Loss: 4702760599552.000 | Tiempo: 1906.8s\n[Log] Paso 12600: Loss final = 4702760599552.0000\nIteración: 12700 | Loss: 83599294464.000 | Tiempo: 1917.5s\n[Log] Paso 12700: Loss final = 83599294464.0000\nIteración: 12800 | Loss: 89763872768.000 | Tiempo: 1928.1s\n[Log] Paso 12800: Loss final = 89763872768.0000\nIteración: 12900 | Loss: 10236301672448.000 | Tiempo: 1938.7s\n[Log] Paso 12900: Loss final = 10236301672448.0000\nIteración: 13000 | Loss: 108739428352.000 | Tiempo: 1949.3s\n[Log] Paso 13000: Loss final = 108739428352.0000\n\n--- EVALUACIÓN Paso 13000: Puntos Promedio = 16.600000381469727 ---\nIteración: 13100 | Loss: 110705508352.000 | Tiempo: 2003.5s\n[Log] Paso 13100: Loss final = 110705508352.0000\nIteración: 13200 | Loss: 110783102976.000 | Tiempo: 2014.2s\n[Log] Paso 13200: Loss final = 110783102976.0000\nIteración: 13300 | Loss: 119510401024.000 | Tiempo: 2024.9s\n[Log] Paso 13300: Loss final = 119510401024.0000\nIteración: 13400 | Loss: 115851395072.000 | Tiempo: 2035.6s\n[Log] Paso 13400: Loss final = 115851395072.0000\nIteración: 13500 | Loss: 144375808000.000 | Tiempo: 2046.4s\n[Log] Paso 13500: Loss final = 144375808000.0000\nIteración: 13600 | Loss: 136517779456.000 | Tiempo: 2057.1s\n[Log] Paso 13600: Loss final = 136517779456.0000\nIteración: 13700 | Loss: 122785628160.000 | Tiempo: 2067.7s\n[Log] Paso 13700: Loss final = 122785628160.0000\nIteración: 13800 | Loss: 137309454336.000 | Tiempo: 2078.7s\n[Log] Paso 13800: Loss final = 137309454336.0000\nIteración: 13900 | Loss: 154988445696.000 | Tiempo: 2089.5s\n[Log] Paso 13900: Loss final = 154988445696.0000\nIteración: 14000 | Loss: 160311017472.000 | Tiempo: 2100.1s\n[Log] Paso 14000: Loss final = 160311017472.0000\n\n--- EVALUACIÓN Paso 14000: Puntos Promedio = 0.0 ---\nIteración: 14100 | Loss: 147924189184.000 | Tiempo: 2154.0s\n[Log] Paso 14100: Loss final = 147924189184.0000\nIteración: 14200 | Loss: 149155741696.000 | Tiempo: 2164.8s\n[Log] Paso 14200: Loss final = 149155741696.0000\nIteración: 14300 | Loss: 175743959040.000 | Tiempo: 2175.7s\n[Log] Paso 14300: Loss final = 175743959040.0000\nIteración: 14400 | Loss: 137302638592.000 | Tiempo: 2186.5s\n[Log] Paso 14400: Loss final = 137302638592.0000\nIteración: 14500 | Loss: 168493056000.000 | Tiempo: 2197.3s\n[Log] Paso 14500: Loss final = 168493056000.0000\nIteración: 14600 | Loss: 8605873471488.000 | Tiempo: 2208.1s\n[Log] Paso 14600: Loss final = 8605873471488.0000\nIteración: 14700 | Loss: 164411473920.000 | Tiempo: 2218.9s\n[Log] Paso 14700: Loss final = 164411473920.0000\nIteración: 14800 | Loss: 165089902592.000 | Tiempo: 2229.9s\n[Log] Paso 14800: Loss final = 165089902592.0000\nIteración: 14900 | Loss: 165449564160.000 | Tiempo: 2240.7s\n[Log] Paso 14900: Loss final = 165449564160.0000\nIteración: 15000 | Loss: 169017868288.000 | Tiempo: 2251.3s\n[Log] Paso 15000: Loss final = 169017868288.0000\n\n--- EVALUACIÓN Paso 15000: Puntos Promedio = 0.0 ---\nIteración: 15100 | Loss: 173535133696.000 | Tiempo: 2305.7s\n[Log] Paso 15100: Loss final = 173535133696.0000\nIteración: 15200 | Loss: 214561718272.000 | Tiempo: 2316.4s\n[Log] Paso 15200: Loss final = 214561718272.0000\nIteración: 15300 | Loss: 10504438284288.000 | Tiempo: 2327.2s\n[Log] Paso 15300: Loss final = 10504438284288.0000\nIteración: 15400 | Loss: 271284436992.000 | Tiempo: 2338.0s\n[Log] Paso 15400: Loss final = 271284436992.0000\nIteración: 15500 | Loss: 11057895571456.000 | Tiempo: 2348.7s\n[Log] Paso 15500: Loss final = 11057895571456.0000\nIteración: 15600 | Loss: 233104736256.000 | Tiempo: 2359.4s\n[Log] Paso 15600: Loss final = 233104736256.0000\nIteración: 15700 | Loss: 211123437568.000 | Tiempo: 2370.1s\n[Log] Paso 15700: Loss final = 211123437568.0000\nIteración: 15800 | Loss: 243474104320.000 | Tiempo: 2380.7s\n[Log] Paso 15800: Loss final = 243474104320.0000\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"iterations = range(0, num_iterations + 1, eval_interval)\nplt.plot(iterations, returns)\nplt.ylabel('Puntos (Cruces exitosos)')\nplt.xlabel('Iteraciones')\nplt.title(\"Entrenamiento DQN en Freeway\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resultados y Conclusiones del Entrenamiento en Freeway\n\n1. **Oscilaciones iniciales**:  \n   - Durante gran parte del entrenamiento, el retorno promedio presenta subidas y bajadas bruscas.  \n   - Esto refleja la exploración del agente: a veces cruza rápidamente la autopista y otras veces se queda dudando, probando distintas estrategias.\n\n2. **Estabilización del comportamiento**:  \n   - Con el avance del entrenamiento, las oscilaciones disminuyen.  \n   - El retorno promedio se mantiene en un nivel alto y constante, indicando que el agente ha aprendido una estrategia confiable.\n\n3. **Resultado final**:  \n   - El agente logra cruzar la autopista repetidamente de manera segura.  \n   - Esto demuestra que ha aprendido a interpretar el tráfico y a tomar decisiones eficientes en el entorno de Freeway.\n\n**Conclusión**:  \nEl entrenamiento fue exitoso: el agente pasa de un comportamiento errático a uno estable y eficiente, cumpliendo el objetivo del juego.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport imageio\nimport IPython\nimport base64\nimport sys\n\ndef find_ale_and_grab_screen(env_obj):\n    \"\"\"Busca el objeto ALE (el emulador) recursivamente y extrae la pantalla.\"\"\"\n    \n    # 1. Si este objeto tiene 'ale', ¡PREMIO!\n    if hasattr(env_obj, 'ale'):\n        h, w = 210, 160\n        buffer = np.empty((h, w, 3), dtype=np.uint8)\n        env_obj.ale.getScreenRGB(buffer)\n        return buffer\n\n    # 2. Si tiene 'env', seguimos cavando más profundo\n    if hasattr(env_obj, 'env'):\n        return find_ale_and_grab_screen(env_obj.env)\n    \n    # 3. Si tiene 'envs' (es un vector de entornos), miramos el primero\n    if hasattr(env_obj, 'envs'):\n        return find_ale_and_grab_screen(env_obj.envs[0])\n\n    # 4. Si tiene 'pyenv' (es un wrapper de TF), miramos dentro\n    if hasattr(env_obj, 'pyenv'):\n        return find_ale_and_grab_screen(env_obj.pyenv)\n\n    # Si llegamos aquí, no encontramos nada :(\n    return None\n\ndef robust_render(env):\n    # Intentamos encontrar el emulador y sacar la foto\n    frame = find_ale_and_grab_screen(env)\n    \n    if frame is not None:\n        return frame\n    \n    # Si falla, imprimimos aviso (solo la primera vez para no spamear)\n    if not hasattr(robust_render, \"warned\"):\n        print(\" No se encontró la interfaz ALE. Usando fallback negro.\")\n        robust_render.warned = True\n        \n    return np.zeros((210, 160, 3), dtype=np.uint8)\n\n# --- GENERACIÓN DEL VIDEO ---\ndef create_policy_eval_video(policy, filename, num_episodes=3, fps=30):\n  filename = filename + \".mp4\"\n  with imageio.get_writer(filename, fps=fps) as video:\n    for ep in range(num_episodes):\n      \n      # Usamos eval_env para resetear (necesario para el agente)\n      time_step = eval_env.reset()\n      \n      # Intentamos renderizar. IMPORTANTE: Pasamos 'py_env_eval' también como pista\n      # ya que a veces es más fácil encontrar el emulador desde el entorno Python puro.\n      frame = robust_render(py_env_eval) \n      if np.all(frame == 0): # Si falló con py_env, probamos con eval_env\n          frame = robust_render(eval_env)\n          \n      video.append_data(frame)\n\n      while not time_step.is_last():\n        action_step = policy.action(time_step)\n        time_step = eval_env.step(action_step.action)\n\n        frame = robust_render(py_env_eval)\n        if np.all(frame == 0): \n            frame = robust_render(eval_env)\n        video.append_data(frame)\n        \n    print(f\"Video generado con éxito ({num_episodes} episodios).\")\n  return filename\n\ndef embed_mp4(filename):\n  video = open(filename, 'rb').read()\n  b64 = base64.b64encode(video)\n  tag = '''\n  <video width=\"640\" height=\"480\" controls>\n    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n  </video>'''.format(b64.decode())\n  return IPython.display.HTML(tag)\n\nprint(\"Generando video del pollo cruzando la calle...\")\nvideo_filename = create_policy_eval_video(eval_policy, \"freeway_agent\")\nprint(f\"Video listo: {video_filename}\")\nembed_mp4(video_filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}